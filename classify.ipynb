{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Mara Fennema\n",
    "# Code inspired by https://www.kaggle.com/ahmethamzaemra/mlpclassifier-example\n",
    "# \n",
    "# This notebook classifies a dataset of files on their focus. \n",
    "# It does so based on the cell of the hidden layer of the LSTM\n",
    "# used for predicting pitch. \n",
    "# It randomly creates the train and testset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from subprocess import check_output\n",
    "import ast\n",
    "path = \"PATH-TO-LABELED-HIDDEN-LAYERS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hidden</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[[1.068283200263977], [1.0058785676956177], [...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[[-0.28725481033325195], [0.7916898131370544]...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[[[0.593345582485199], [-2.0425736904144287], ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[[-0.5890960097312927], [-0.7134659886360168]...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[[[0.6272860169410706], [0.9330867528915405], ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             hidden  class\n",
       "0           0  [[[1.068283200263977], [1.0058785676956177], [...      1\n",
       "1           1  [[[-0.28725481033325195], [0.7916898131370544]...      2\n",
       "2           2  [[[0.593345582485199], [-2.0425736904144287], ...      0\n",
       "3           3  [[[-0.5890960097312927], [-0.7134659886360168]...      1\n",
       "4           4  [[[0.6272860169410706], [0.9330867528915405], ...      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumdf = df.copy()\n",
    "meandf = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Gives a warning as values in the dataframe are changed. \n",
    "for i in range(0, len(sumdf['hidden'])):\n",
    "    sumdf['hidden'][i] = np.sum(ast.literal_eval(sumdf['hidden'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hidden</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.805177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.46735</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.11513</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-5.50461</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-2.96285</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    hidden  class\n",
       "0           0  0.805177      1\n",
       "1           1  -1.46735      2\n",
       "2           2  -1.11513      0\n",
       "3           3  -5.50461      1\n",
       "4           4  -2.96285      2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Gives a warning as values in the dataframe are changed. \n",
    "for i in range(0, len(meandf['hidden'])):\n",
    "    meandf['hidden'][i] = np.mean(ast.literal_eval(meandf['hidden'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hidden</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0251618</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.0458547</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.0348478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.172019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.0925891</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     hidden  class\n",
       "0           0  0.0251618      1\n",
       "1           1 -0.0458547      2\n",
       "2           2 -0.0348478      0\n",
       "3           3  -0.172019      1\n",
       "4           4 -0.0925891      2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meandf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the train and testset for the sum-implementation\n",
    "y_sum = sumdf['class']\n",
    "x_sum = sumdf.drop(['class'], axis=1)\n",
    "\n",
    "x_train_sum, x_test_sum, y_train_sum, y_test_sum = train_test_split(x_sum,y_sum, test_size= 0.2)\n",
    "\n",
    "clf_sum = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001,\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the train and testset for the mean-implementation\n",
    "y_mean = meandf['class']\n",
    "x_mean = meandf.drop(['class'], axis=1)\n",
    "\n",
    "x_train_mean, x_test_mean, y_train_mean, y_test_mean = train_test_split(x_mean,y_mean, test_size= 0.2)\n",
    "\n",
    "clf_mean = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001,\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 9.40759923\n",
      "Iteration 2, loss = 2.43331607\n",
      "Iteration 3, loss = 1.41866261\n",
      "Iteration 4, loss = 1.15000643\n",
      "Iteration 5, loss = 1.10881673\n",
      "Iteration 6, loss = 1.10405613\n",
      "Iteration 7, loss = 1.10218429\n",
      "Iteration 8, loss = 1.10152561\n",
      "Iteration 9, loss = 1.10136693\n",
      "Iteration 10, loss = 1.10126228\n",
      "Iteration 11, loss = 1.10103764\n",
      "Iteration 12, loss = 1.10094699\n",
      "Iteration 13, loss = 1.10085564\n",
      "Iteration 14, loss = 1.10076033\n",
      "Iteration 15, loss = 1.10068773\n",
      "Iteration 16, loss = 1.10066878\n",
      "Iteration 17, loss = 1.10051859\n",
      "Iteration 18, loss = 1.10056128\n",
      "Iteration 19, loss = 1.10048168\n",
      "Iteration 20, loss = 1.10045193\n",
      "Iteration 21, loss = 1.10041830\n",
      "Iteration 22, loss = 1.10046106\n",
      "Iteration 23, loss = 1.10032227\n",
      "Iteration 24, loss = 1.10043696\n",
      "Iteration 25, loss = 1.10035708\n",
      "Iteration 26, loss = 1.10028446\n",
      "Iteration 27, loss = 1.10029287\n",
      "Iteration 28, loss = 1.10027139\n",
      "Iteration 29, loss = 1.10026450\n",
      "Iteration 30, loss = 1.10016904\n",
      "Iteration 31, loss = 1.10024502\n",
      "Iteration 32, loss = 1.10015696\n",
      "Iteration 33, loss = 1.10025514\n",
      "Iteration 34, loss = 1.10016053\n",
      "Iteration 35, loss = 1.10005211\n",
      "Iteration 36, loss = 1.10009993\n",
      "Iteration 37, loss = 1.10007238\n",
      "Iteration 38, loss = 1.10019939\n",
      "Iteration 39, loss = 1.09997784\n",
      "Iteration 40, loss = 1.09998461\n",
      "Iteration 41, loss = 1.10003030\n",
      "Iteration 42, loss = 1.10006626\n",
      "Iteration 43, loss = 1.09998296\n",
      "Iteration 44, loss = 1.10018494\n",
      "Iteration 45, loss = 1.09992234\n",
      "Iteration 46, loss = 1.09992415\n",
      "Iteration 47, loss = 1.09992840\n",
      "Iteration 48, loss = 1.09986533\n",
      "Iteration 49, loss = 1.10004813\n",
      "Iteration 50, loss = 1.09987087\n",
      "Iteration 51, loss = 1.09982298\n",
      "Iteration 52, loss = 1.09979433\n",
      "Iteration 53, loss = 1.09978563\n",
      "Iteration 54, loss = 1.09978687\n",
      "Iteration 55, loss = 1.09977570\n",
      "Iteration 56, loss = 1.09974689\n",
      "Iteration 57, loss = 1.09980564\n",
      "Iteration 58, loss = 1.09992333\n",
      "Iteration 59, loss = 1.09978221\n",
      "Iteration 60, loss = 1.09975822\n",
      "Iteration 61, loss = 1.09970527\n",
      "Iteration 62, loss = 1.09971596\n",
      "Iteration 63, loss = 1.09964932\n",
      "Iteration 64, loss = 1.09965349\n",
      "Iteration 65, loss = 1.09960494\n",
      "Iteration 66, loss = 1.09970410\n",
      "Iteration 67, loss = 1.09974880\n",
      "Iteration 68, loss = 1.09980434\n",
      "Iteration 69, loss = 1.09962037\n",
      "Iteration 70, loss = 1.09962225\n",
      "Iteration 71, loss = 1.09960389\n",
      "Iteration 72, loss = 1.09977118\n",
      "Iteration 73, loss = 1.09963138\n",
      "Iteration 74, loss = 1.09958368\n",
      "Iteration 75, loss = 1.09953672\n",
      "Iteration 76, loss = 1.09955372\n",
      "Iteration 77, loss = 1.09950359\n",
      "Iteration 78, loss = 1.09951912\n",
      "Iteration 79, loss = 1.09948287\n",
      "Iteration 80, loss = 1.09955516\n",
      "Iteration 81, loss = 1.09953280\n",
      "Iteration 82, loss = 1.09943711\n",
      "Iteration 83, loss = 1.09945466\n",
      "Iteration 84, loss = 1.09945100\n",
      "Iteration 85, loss = 1.09941920\n",
      "Iteration 86, loss = 1.09953879\n",
      "Iteration 87, loss = 1.09942461\n",
      "Iteration 88, loss = 1.09944140\n",
      "Iteration 89, loss = 1.09947947\n",
      "Iteration 90, loss = 1.09943424\n",
      "Iteration 91, loss = 1.09947341\n",
      "Iteration 92, loss = 1.09933702\n",
      "Iteration 93, loss = 1.09929417\n",
      "Iteration 94, loss = 1.09939706\n",
      "Iteration 95, loss = 1.09934668\n",
      "Iteration 96, loss = 1.09928518\n",
      "Iteration 97, loss = 1.09924231\n",
      "Iteration 98, loss = 1.09928892\n",
      "Iteration 99, loss = 1.09927050\n",
      "Iteration 100, loss = 1.09926921\n",
      "Iteration 101, loss = 1.09928322\n",
      "Iteration 102, loss = 1.09923856\n",
      "Iteration 103, loss = 1.09921806\n",
      "Iteration 104, loss = 1.09926391\n",
      "Iteration 105, loss = 1.09923811\n",
      "Iteration 106, loss = 1.09938729\n",
      "Iteration 107, loss = 1.09918910\n",
      "Iteration 108, loss = 1.09914695\n",
      "Iteration 109, loss = 1.09933606\n",
      "Iteration 110, loss = 1.09931364\n",
      "Iteration 111, loss = 1.09920358\n",
      "Iteration 112, loss = 1.09912759\n",
      "Iteration 113, loss = 1.09911989\n",
      "Iteration 114, loss = 1.09908902\n",
      "Iteration 115, loss = 1.09920692\n",
      "Iteration 116, loss = 1.09909310\n",
      "Iteration 117, loss = 1.09907351\n",
      "Iteration 118, loss = 1.09912506\n",
      "Iteration 119, loss = 1.09911335\n",
      "Iteration 120, loss = 1.09908291\n",
      "Iteration 121, loss = 1.09908785\n",
      "Iteration 122, loss = 1.09910426\n",
      "Iteration 123, loss = 1.09900620\n",
      "Iteration 124, loss = 1.09909187\n",
      "Iteration 125, loss = 1.09902920\n",
      "Iteration 126, loss = 1.09921649\n",
      "Iteration 127, loss = 1.09912798\n",
      "Iteration 128, loss = 1.09899331\n",
      "Iteration 129, loss = 1.09908517\n",
      "Iteration 130, loss = 1.09899808\n",
      "Iteration 131, loss = 1.09894359\n",
      "Iteration 132, loss = 1.09899692\n",
      "Iteration 133, loss = 1.09896386\n",
      "Iteration 134, loss = 1.09891864\n",
      "Iteration 135, loss = 1.09890967\n",
      "Iteration 136, loss = 1.09897252\n",
      "Iteration 137, loss = 1.09893372\n",
      "Iteration 138, loss = 1.09903025\n",
      "Iteration 139, loss = 1.09906160\n",
      "Iteration 140, loss = 1.09891950\n",
      "Iteration 141, loss = 1.09891299\n",
      "Iteration 142, loss = 1.09888040\n",
      "Iteration 143, loss = 1.09889449\n",
      "Iteration 144, loss = 1.09885142\n",
      "Iteration 145, loss = 1.09886340\n",
      "Iteration 146, loss = 1.09884040\n",
      "Iteration 147, loss = 1.09884625\n",
      "Iteration 148, loss = 1.09882463\n",
      "Iteration 149, loss = 1.09879125\n",
      "Iteration 150, loss = 1.09888290\n",
      "Iteration 151, loss = 1.09885459\n",
      "Iteration 152, loss = 1.09883953\n",
      "Iteration 153, loss = 1.09881691\n",
      "Iteration 154, loss = 1.09877776\n",
      "Iteration 155, loss = 1.09882531\n",
      "Iteration 156, loss = 1.09889373\n",
      "Iteration 157, loss = 1.09872592\n",
      "Iteration 158, loss = 1.09908624\n",
      "Iteration 159, loss = 1.09869183\n",
      "Iteration 160, loss = 1.09877470\n",
      "Iteration 161, loss = 1.09877201\n",
      "Iteration 162, loss = 1.09871162\n",
      "Iteration 163, loss = 1.09880224\n",
      "Iteration 164, loss = 1.09876305\n",
      "Iteration 165, loss = 1.09868812\n",
      "Iteration 166, loss = 1.09874005\n",
      "Iteration 167, loss = 1.09871145\n",
      "Iteration 168, loss = 1.09880956\n",
      "Iteration 169, loss = 1.09869081\n",
      "Iteration 170, loss = 1.09864608\n",
      "Iteration 171, loss = 1.09879600\n",
      "Iteration 172, loss = 1.09869381\n",
      "Iteration 173, loss = 1.09860661\n",
      "Iteration 174, loss = 1.09862345\n",
      "Iteration 175, loss = 1.09868124\n",
      "Iteration 176, loss = 1.09861955\n",
      "Iteration 177, loss = 1.09866280\n",
      "Iteration 178, loss = 1.09859264\n",
      "Iteration 179, loss = 1.09857307\n",
      "Iteration 180, loss = 1.09854201\n",
      "Iteration 181, loss = 1.09869171\n",
      "Iteration 182, loss = 1.09857780\n",
      "Iteration 183, loss = 1.09855010\n",
      "Iteration 184, loss = 1.09853329\n",
      "Iteration 185, loss = 1.09853907\n",
      "Iteration 186, loss = 1.09847674\n",
      "Iteration 187, loss = 1.09858941\n",
      "Iteration 188, loss = 1.09851156\n",
      "Iteration 189, loss = 1.09852234\n",
      "Iteration 190, loss = 1.09853541\n",
      "Iteration 191, loss = 1.09860294\n",
      "Iteration 192, loss = 1.09855852\n",
      "Iteration 193, loss = 1.09847223\n",
      "Iteration 194, loss = 1.09849198\n",
      "Iteration 195, loss = 1.09864866\n",
      "Iteration 196, loss = 1.09852590\n",
      "Iteration 197, loss = 1.09843896\n",
      "Iteration 198, loss = 1.09842547\n",
      "Iteration 199, loss = 1.09841062\n",
      "Iteration 200, loss = 1.09847302\n",
      "Iteration 201, loss = 1.09844189\n",
      "Iteration 202, loss = 1.09845710\n",
      "Iteration 203, loss = 1.09841247\n",
      "Iteration 204, loss = 1.09839290\n",
      "Iteration 205, loss = 1.09850022\n",
      "Iteration 206, loss = 1.09832781\n",
      "Iteration 207, loss = 1.09841087\n",
      "Iteration 208, loss = 1.09840337\n",
      "Iteration 209, loss = 1.09836050\n",
      "Iteration 210, loss = 1.09844515\n",
      "Iteration 211, loss = 1.09833764\n",
      "Iteration 212, loss = 1.09841453\n",
      "Iteration 213, loss = 1.09835179\n",
      "Iteration 214, loss = 1.09834586\n",
      "Iteration 215, loss = 1.09831518\n",
      "Iteration 216, loss = 1.09836590\n",
      "Iteration 217, loss = 1.09833958\n",
      "Iteration 218, loss = 1.09830469\n",
      "Iteration 219, loss = 1.09833310\n",
      "Iteration 220, loss = 1.09832826\n",
      "Iteration 221, loss = 1.09827830\n",
      "Iteration 222, loss = 1.09843754\n",
      "Iteration 223, loss = 1.09837403\n",
      "Iteration 224, loss = 1.09828217\n",
      "Iteration 225, loss = 1.09837334\n",
      "Iteration 226, loss = 1.09823228\n",
      "Iteration 227, loss = 1.09827093\n",
      "Iteration 228, loss = 1.09826715\n",
      "Iteration 229, loss = 1.09825499\n",
      "Iteration 230, loss = 1.09825661\n",
      "Iteration 231, loss = 1.09822117\n",
      "Iteration 232, loss = 1.09825545\n",
      "Iteration 233, loss = 1.09822706\n",
      "Iteration 234, loss = 1.09820879\n",
      "Iteration 235, loss = 1.09825463\n",
      "Iteration 236, loss = 1.09821989\n",
      "Iteration 237, loss = 1.09823356\n",
      "Iteration 238, loss = 1.09823861\n",
      "Iteration 239, loss = 1.09821256\n",
      "Iteration 240, loss = 1.09827423\n",
      "Iteration 241, loss = 1.09820536\n",
      "Iteration 242, loss = 1.09819500\n",
      "Iteration 243, loss = 1.09817093\n",
      "Iteration 244, loss = 1.09827344\n",
      "Iteration 245, loss = 1.09827302\n",
      "Iteration 246, loss = 1.09825327\n",
      "Iteration 247, loss = 1.09817862\n",
      "Iteration 248, loss = 1.09834262\n",
      "Iteration 249, loss = 1.09825478\n",
      "Iteration 250, loss = 1.09821252\n",
      "Iteration 251, loss = 1.09823984\n",
      "Iteration 252, loss = 1.09815957\n",
      "Iteration 253, loss = 1.09813061\n",
      "Iteration 254, loss = 1.09815007\n",
      "Iteration 255, loss = 1.09813819\n",
      "Iteration 256, loss = 1.09837228\n",
      "Iteration 257, loss = 1.09824420\n",
      "Iteration 258, loss = 1.09816834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 259, loss = 1.09816479\n",
      "Iteration 260, loss = 1.09817980\n",
      "Iteration 261, loss = 1.09816427\n",
      "Iteration 262, loss = 1.09810220\n",
      "Iteration 263, loss = 1.09808686\n",
      "Iteration 264, loss = 1.09814853\n",
      "Iteration 265, loss = 1.09818041\n",
      "Iteration 266, loss = 1.09817692\n",
      "Iteration 267, loss = 1.09812510\n",
      "Iteration 268, loss = 1.09817288\n",
      "Iteration 269, loss = 1.09812822\n",
      "Iteration 270, loss = 1.09812334\n",
      "Iteration 271, loss = 1.09805324\n",
      "Iteration 272, loss = 1.09805642\n",
      "Iteration 273, loss = 1.09806989\n",
      "Iteration 274, loss = 1.09806002\n",
      "Iteration 275, loss = 1.09822972\n",
      "Iteration 276, loss = 1.09819536\n",
      "Iteration 277, loss = 1.09809328\n",
      "Iteration 278, loss = 1.09808609\n",
      "Iteration 279, loss = 1.09805218\n",
      "Iteration 280, loss = 1.09803947\n",
      "Iteration 281, loss = 1.09829160\n",
      "Iteration 282, loss = 1.09799881\n",
      "Iteration 283, loss = 1.09806440\n",
      "Iteration 284, loss = 1.09809783\n",
      "Iteration 285, loss = 1.09802584\n",
      "Iteration 286, loss = 1.09804741\n",
      "Iteration 287, loss = 1.09810912\n",
      "Iteration 288, loss = 1.09807379\n",
      "Iteration 289, loss = 1.09798618\n",
      "Iteration 290, loss = 1.09813527\n",
      "Iteration 291, loss = 1.09799996\n",
      "Iteration 292, loss = 1.09806520\n",
      "Iteration 293, loss = 1.09804238\n",
      "Iteration 294, loss = 1.09805026\n",
      "Iteration 295, loss = 1.09798858\n",
      "Iteration 296, loss = 1.09808166\n",
      "Iteration 297, loss = 1.09799594\n",
      "Iteration 298, loss = 1.09798909\n",
      "Iteration 299, loss = 1.09798997\n",
      "Iteration 300, loss = 1.09795608\n",
      "Iteration 301, loss = 1.09795557\n",
      "Iteration 302, loss = 1.09804114\n",
      "Iteration 303, loss = 1.09795496\n",
      "Iteration 304, loss = 1.09803891\n",
      "Iteration 305, loss = 1.09795657\n",
      "Iteration 306, loss = 1.09807080\n",
      "Iteration 307, loss = 1.09799472\n",
      "Iteration 308, loss = 1.09798505\n",
      "Iteration 309, loss = 1.09800630\n",
      "Iteration 310, loss = 1.09794784\n",
      "Iteration 311, loss = 1.09795903\n",
      "Iteration 312, loss = 1.09801770\n",
      "Iteration 313, loss = 1.09796948\n",
      "Iteration 314, loss = 1.09792277\n",
      "Iteration 315, loss = 1.09802780\n",
      "Iteration 316, loss = 1.09795456\n",
      "Iteration 317, loss = 1.09788122\n",
      "Iteration 318, loss = 1.09797874\n",
      "Iteration 319, loss = 1.09793002\n",
      "Iteration 320, loss = 1.09795661\n",
      "Iteration 321, loss = 1.09792555\n",
      "Iteration 322, loss = 1.09809470\n",
      "Iteration 323, loss = 1.09798434\n",
      "Iteration 324, loss = 1.09800061\n",
      "Iteration 325, loss = 1.09798700\n",
      "Iteration 326, loss = 1.09786135\n",
      "Iteration 327, loss = 1.09791392\n",
      "Iteration 328, loss = 1.09790920\n",
      "Iteration 329, loss = 1.09799954\n",
      "Iteration 330, loss = 1.09789586\n",
      "Iteration 331, loss = 1.09783675\n",
      "Iteration 332, loss = 1.09794516\n",
      "Iteration 333, loss = 1.09786489\n",
      "Iteration 334, loss = 1.09792023\n",
      "Iteration 335, loss = 1.09786909\n",
      "Iteration 336, loss = 1.09793200\n",
      "Iteration 337, loss = 1.09795982\n",
      "Iteration 338, loss = 1.09784160\n",
      "Iteration 339, loss = 1.09782518\n",
      "Iteration 340, loss = 1.09789356\n",
      "Iteration 341, loss = 1.09788143\n",
      "Iteration 342, loss = 1.09785755\n",
      "Iteration 343, loss = 1.09781987\n",
      "Iteration 344, loss = 1.09792645\n",
      "Iteration 345, loss = 1.09782587\n",
      "Iteration 346, loss = 1.09789419\n",
      "Iteration 347, loss = 1.09784682\n",
      "Iteration 348, loss = 1.09783857\n",
      "Iteration 349, loss = 1.09783850\n",
      "Iteration 350, loss = 1.09782921\n",
      "Iteration 351, loss = 1.09780899\n",
      "Iteration 352, loss = 1.09786269\n",
      "Iteration 353, loss = 1.09790171\n",
      "Iteration 354, loss = 1.09780368\n",
      "Iteration 355, loss = 1.09782555\n",
      "Iteration 356, loss = 1.09789169\n",
      "Iteration 357, loss = 1.09779789\n",
      "Iteration 358, loss = 1.09781664\n",
      "Iteration 359, loss = 1.09778075\n",
      "Iteration 360, loss = 1.09776003\n",
      "Iteration 361, loss = 1.09792378\n",
      "Iteration 362, loss = 1.09784564\n",
      "Iteration 363, loss = 1.09781697\n",
      "Iteration 364, loss = 1.09782929\n",
      "Iteration 365, loss = 1.09785708\n",
      "Iteration 366, loss = 1.09785170\n",
      "Iteration 367, loss = 1.09782803\n",
      "Iteration 368, loss = 1.09774536\n",
      "Iteration 369, loss = 1.09780007\n",
      "Iteration 370, loss = 1.09772129\n",
      "Iteration 371, loss = 1.09775243\n",
      "Iteration 372, loss = 1.09797143\n",
      "Iteration 373, loss = 1.09773069\n",
      "Iteration 374, loss = 1.09790422\n",
      "Iteration 375, loss = 1.09776046\n",
      "Iteration 376, loss = 1.09777068\n",
      "Iteration 377, loss = 1.09775981\n",
      "Iteration 378, loss = 1.09782756\n",
      "Iteration 379, loss = 1.09794589\n",
      "Iteration 380, loss = 1.09775988\n",
      "Iteration 381, loss = 1.09798816\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "clf_sum.fit(x_train_sum, y_train_sum)\n",
    "y_pred_sum= clf_sum.predict(x_test_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 10.66857869\n",
      "Iteration 2, loss = 1.63868901\n",
      "Iteration 3, loss = 1.30797240\n",
      "Iteration 4, loss = 1.14796275\n",
      "Iteration 5, loss = 1.10424978\n",
      "Iteration 6, loss = 1.10151857\n",
      "Iteration 7, loss = 1.10179043\n",
      "Iteration 8, loss = 1.10065957\n",
      "Iteration 9, loss = 1.10028522\n",
      "Iteration 10, loss = 1.10047208\n",
      "Iteration 11, loss = 1.10046266\n",
      "Iteration 12, loss = 1.10045301\n",
      "Iteration 13, loss = 1.10003442\n",
      "Iteration 14, loss = 1.10008639\n",
      "Iteration 15, loss = 1.10002899\n",
      "Iteration 16, loss = 1.10025740\n",
      "Iteration 17, loss = 1.10002005\n",
      "Iteration 18, loss = 1.10025715\n",
      "Iteration 19, loss = 1.09986815\n",
      "Iteration 20, loss = 1.10001028\n",
      "Iteration 21, loss = 1.10007064\n",
      "Iteration 22, loss = 1.10001606\n",
      "Iteration 23, loss = 1.09984756\n",
      "Iteration 24, loss = 1.10024917\n",
      "Iteration 25, loss = 1.09997851\n",
      "Iteration 26, loss = 1.10000319\n",
      "Iteration 27, loss = 1.09992617\n",
      "Iteration 28, loss = 1.09994274\n",
      "Iteration 29, loss = 1.10006210\n",
      "Iteration 30, loss = 1.09979439\n",
      "Iteration 31, loss = 1.10012155\n",
      "Iteration 32, loss = 1.09974314\n",
      "Iteration 33, loss = 1.09995356\n",
      "Iteration 34, loss = 1.10033930\n",
      "Iteration 35, loss = 1.09983079\n",
      "Iteration 36, loss = 1.09990781\n",
      "Iteration 37, loss = 1.09971444\n",
      "Iteration 38, loss = 1.09999227\n",
      "Iteration 39, loss = 1.09977205\n",
      "Iteration 40, loss = 1.09978241\n",
      "Iteration 41, loss = 1.09985207\n",
      "Iteration 42, loss = 1.09977925\n",
      "Iteration 43, loss = 1.09977493\n",
      "Iteration 44, loss = 1.09972902\n",
      "Iteration 45, loss = 1.09963334\n",
      "Iteration 46, loss = 1.09974315\n",
      "Iteration 47, loss = 1.09975004\n",
      "Iteration 48, loss = 1.09961889\n",
      "Iteration 49, loss = 1.09962946\n",
      "Iteration 50, loss = 1.09984895\n",
      "Iteration 51, loss = 1.09963430\n",
      "Iteration 52, loss = 1.09963087\n",
      "Iteration 53, loss = 1.09950421\n",
      "Iteration 54, loss = 1.09965309\n",
      "Iteration 55, loss = 1.09952842\n",
      "Iteration 56, loss = 1.09948706\n",
      "Iteration 57, loss = 1.09965100\n",
      "Iteration 58, loss = 1.09961501\n",
      "Iteration 59, loss = 1.09967307\n",
      "Iteration 60, loss = 1.09962459\n",
      "Iteration 61, loss = 1.09944953\n",
      "Iteration 62, loss = 1.09946766\n",
      "Iteration 63, loss = 1.09983868\n",
      "Iteration 64, loss = 1.09970424\n",
      "Iteration 65, loss = 1.09951340\n",
      "Iteration 66, loss = 1.09963228\n",
      "Iteration 67, loss = 1.09958452\n",
      "Iteration 68, loss = 1.09956866\n",
      "Iteration 69, loss = 1.09960876\n",
      "Iteration 70, loss = 1.09934147\n",
      "Iteration 71, loss = 1.09934332\n",
      "Iteration 72, loss = 1.09934224\n",
      "Iteration 73, loss = 1.09923449\n",
      "Iteration 74, loss = 1.09958532\n",
      "Iteration 75, loss = 1.09975529\n",
      "Iteration 76, loss = 1.09968476\n",
      "Iteration 77, loss = 1.09948225\n",
      "Iteration 78, loss = 1.09931835\n",
      "Iteration 79, loss = 1.09941110\n",
      "Iteration 80, loss = 1.09940852\n",
      "Iteration 81, loss = 1.09945918\n",
      "Iteration 82, loss = 1.09931140\n",
      "Iteration 83, loss = 1.09928752\n",
      "Iteration 84, loss = 1.09915068\n",
      "Iteration 85, loss = 1.09926263\n",
      "Iteration 86, loss = 1.09926318\n",
      "Iteration 87, loss = 1.09929800\n",
      "Iteration 88, loss = 1.09948150\n",
      "Iteration 89, loss = 1.09926100\n",
      "Iteration 90, loss = 1.09940537\n",
      "Iteration 91, loss = 1.09921172\n",
      "Iteration 92, loss = 1.09955404\n",
      "Iteration 93, loss = 1.09925859\n",
      "Iteration 94, loss = 1.09940240\n",
      "Iteration 95, loss = 1.09900975\n",
      "Iteration 96, loss = 1.09913517\n",
      "Iteration 97, loss = 1.09925026\n",
      "Iteration 98, loss = 1.09921707\n",
      "Iteration 99, loss = 1.09918348\n",
      "Iteration 100, loss = 1.09923566\n",
      "Iteration 101, loss = 1.09906579\n",
      "Iteration 102, loss = 1.09953453\n",
      "Iteration 103, loss = 1.09910222\n",
      "Iteration 104, loss = 1.09947598\n",
      "Iteration 105, loss = 1.09934935\n",
      "Iteration 106, loss = 1.09911677\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "clf_mean.fit(x_train_mean, y_train_mean)\n",
    "y_pred_mean= clf_mean.predict(x_test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_sum' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-73e841e2d5bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_train_sum\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlo\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train_sum' is not defined"
     ]
    }
   ],
   "source": [
    "# Prints how much of each class is in each train and testset\n",
    "# for sum implementation\n",
    "\n",
    "lo = 0\n",
    "neu = 0\n",
    "hi = 0\n",
    "\n",
    "for data in y_train_sum:\n",
    "    if data == 0:\n",
    "        lo += 1\n",
    "    elif data == 1:\n",
    "        neu += 1\n",
    "    else:\n",
    "        hi += 1\n",
    "\n",
    "print(\"Number Low Files in sum_train: {}\".format(lo))\n",
    "print(\"Number Neutral Files in sum_train: {}\".format(neu))\n",
    "print(\"Number High Files in sum_train: {}\".format(hi))\n",
    "\n",
    "lo = 0\n",
    "neu = 0\n",
    "hi = 0\n",
    "\n",
    "for data in y_test_sum:\n",
    "    if data == 0:\n",
    "        lo += 1\n",
    "    elif data == 1:\n",
    "        neu += 1\n",
    "    else:\n",
    "        hi += 1\n",
    "\n",
    "print(\"Number Low Files in sum_test: {}\".format(lo))\n",
    "print(\"Number Neutral Files in sum_test: {}\".format(neu))\n",
    "print(\"Number High Files in sum_test: {}\".format(hi))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Low Files in mean_train: 591\n",
      "Number Neutral Files in mean_train: 576\n",
      "Number High Files in mean_train: 569\n",
      "Number Low Files in mean_test: 134\n",
      "Number Neutral Files in mean_test: 148\n",
      "Number High Files in mean_test: 153\n"
     ]
    }
   ],
   "source": [
    "# Prints how much of each class is in each train and testset\n",
    "# for sum implementation\n",
    "\n",
    "lo = 0\n",
    "neu = 0\n",
    "hi = 0\n",
    "\n",
    "for data in y_train_mean:\n",
    "    if data == 0:\n",
    "        lo += 1\n",
    "    elif data == 1:\n",
    "        neu += 1\n",
    "    else:\n",
    "        hi += 1\n",
    "\n",
    "print(\"Number Low Files in mean_train: {}\".format(lo))\n",
    "print(\"Number Neutral Files in mean_train: {}\".format(neu))\n",
    "print(\"Number High Files in mean_train: {}\".format(hi))\n",
    "\n",
    "lo = 0\n",
    "neu = 0\n",
    "hi = 0\n",
    "\n",
    "for data in y_test_mean:\n",
    "    if data == 0:\n",
    "        lo += 1\n",
    "    elif data == 1:\n",
    "        neu += 1\n",
    "    else:\n",
    "        hi += 1\n",
    "\n",
    "print(\"Number Low Files in mean_test: {}\".format(lo))\n",
    "print(\"Number Neutral Files in mean_test: {}\".format(neu))\n",
    "print(\"Number High Files in mean_test: {}\".format(hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28735632183908044"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate accuracy for the sum-implementation\n",
    "accuracy_score(y_test_sum, y_pred_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 157,   0],\n",
       "       [  0, 125,   1],\n",
       "       [  1, 151,   0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the confusion matrix of the sum-implementation\n",
    "cm_sum = confusion_matrix(y_test_sum, y_pred_sum)\n",
    "cm_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the previously calculated confusion matrix as a heatmap\n",
    "sns.heatmap(cm_sum, center=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3080459770114943"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate accuracy for the mean-implementation\n",
    "accuracy_score(y_test_mean, y_pred_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[133,   1,   0],\n",
       "       [148,   0,   0],\n",
       "       [152,   0,   1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the confusion matrix of the mean-implementation\n",
    "cm_mean = confusion_matrix(y_test_mean, y_pred_mean)\n",
    "cm_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the previously calculated confusion matrix as a heatmap\n",
    "sns.heatmap(cm_mean, center=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
